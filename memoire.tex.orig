\documentclass[a4paper, 12pt]{book}
\usepackage{graphicx}
\usepackage[french]{babel}
\usepackage[utf8]{inputenc}
\usepackage{textcomp}
\usepackage[T1]{fontenc}
\usepackage{multirow}

\usepackage[table]{xcolor}

\usepackage{listings}
\usepackage{float}
\usepackage{url}
\usepackage[french]{algorithm}
\usepackage{style/myalgorithm}
\usepackage{amsmath,amsfonts,amssymb}
\usepackage{biblatex}
\addbibresource{memoire}
\newcommand{\fBm}{\emph{fBm}~}
\newcommand{\etal}{\emph{et al.}~}
\newcommand{\glAd}{\emph{GL4D}~}
\newcommand{\apiopengl}{API OpenGL\textsuperscript{\textregistered}~}
\newcommand{\opengl}{OpenGL\textsuperscript{\textregistered}~}
\newcommand{\opengles}{OpenGL\textsuperscript{\textregistered}ES~}
\newcommand{\clang}{langage \texttt{C}}
\newcommand{\codesource}{\textsc{Code source}~}
\floatstyle{ruled}
\newfloat{programslist}{htbp}{locs}
\newcommand{\listofprograms}{\listof{programslist}{Liste des codes source}}
\newcounter{program}[subsection]
\renewcommand{\theprogram}{\arabic{chapter}.\arabic{program}}

\newenvironment{program}[1]{
  \if\relax\detokenize{#1}\relax
  \gdef\mycaption{\relax}
  \else
  \gdef\mycaption{#1}
  \fi
  \refstepcounter{program}
  \addcontentsline{locs}{section}{#1}
  \footnotesize
}{
  \begin{description}
    \item[\codesource \theprogram]--~\mycaption
  \end{description}
}

\begin{document}
\begin{titlepage}
  \begin{center}
    %\begin{tabular*}{\textwidth}{l@{\extracolsep{\fill}}r}
      \includegraphics[height=2.5cm, width=6cm]{images/paris8Logo.png}
    %\end{tabular*}
    \small 
    \rule{\textwidth}{.5pt}~\\
    \large 
    \textsc{Université Paris 8 - Vincennes à Saint-Denis}\vspace{0.5cm}\\
    \textbf{Master 1 Informatique}\vspace{3.0cm}\\
    \Large
    \textbf{Mémoire projet tuteuré}\\
    \textbf{Qualification de caméras RGB-D}\vspace{1.5cm}\\
    
    \large
    \textbf{Yasmine BOUDJEMAÏ}\\
\textbf{Mélanie DE JESUS CORREIA}\vspace{1.5cm}\\
  \end{center}\vspace{3.5cm}~\\
  \begin{tabular}{ll}
    \hspace{-0.45cm}Organisme~:~&~Université Paris 8 Vincennes-Saint-Denis\\
    \hspace{-0.45cm}Tuteur~:~&~Farès  \textsc{BELHADJ}\\
  \end{tabular}
\end{titlepage}
\frontmatter



\chapter*{Dédicaces}
\markboth{\sc Dédicaces}{}


\chapter*{Remerciements}
\markboth{\sc Remerciements}{}

\chapter*{Résumé}
\markboth{\sc Résumé}{}


%% Table des matières
\tableofcontents
%% La liste des figure est optionnelle (si votre rapport manque de
%% contenu ajouter ce type de pages sera perçu négativement)
\listoffigures
%% La liste des programmes est optionnelle (si votre rapport manque de
%% contenu ajouter ce type de pages sera perçu négativement)
%\listofprogram
\mainmatter
\chapter*{Introduction}
\markboth{\sc Introduction}{}
\addcontentsline{toc}{chapter}{Introduction}
Introduction a faire à la fin.
\chapter{État de l'art}

Dans ce chapitre, nous allons définir ce qu'est une caméra RGB-D, nous énumérons certains des différents modèles existants, nous abordons brièvement les différentes techniques utilisées pour la récupération de la profondeur par une caméra. Enfin, nous discutons les différences recensées sur la \texttt{kinect V2} et \texttt{V3}.

\section{Description d'une caméra RGB-D}

La caméra RGB-D, aussi appelée capteur RGB-D, est une caméra fournissant en même temps une image couleur et une carte de profondeur caractérisant la distance des objets vus dans l'image. Cela est rendu possible grâce à un capteur RGB et un capteur de profondeur (D pour Depth). C'est principalement ces captures qui vont nous intéresser tout au lond des qualifications.


\section{Modèles existants}
Il existe différents modèles de caméras RGB-D. Parmi elles, nous pouvons citer la Kinect et ses différentes versions, Asus Xtion Pro Live, BlasterX Senz3D, Orbbec, Intel RealSens D415, ...
\par \texttt{La Kinect} a fait son apparition en septembre 2008. Elle a été conçue par Microsoft et était destinée pour la console de jeu XBox 360. Elle permettait aux utilisateurs d'interagir avec la console à l'aide d'une NUI \footnote{Natural User Interface (Interface Utilisateur Naturelle), se réfère à une interface utilisateur invisible.} en utilisant les mouvements gestuels et une reconnaissance vocale. Elle sera plus tard utilisée dans les domaines de la  recherche et du développement pour différents secteurs comme le domaine de la médecine, l'industrie automobile, la robotique, l'éducation,  .... 
\par \texttt{Asus Xtion Pro Live} est le modèle de référence que nous utilisons afin d'effectuer les qualifications. Elle utilise la technologie PrimeSense  \footnote{Connu principalement pour sa licence de conception matérielle et de puce employée dans le mécanisme de détection de mouvements de la Kinect XBox360. Pour plus d'informations, le lecteur peut se référer à ce lien \url{https://www.crunchbase.com/organization/primesense#section-web-traffic-by-similarweb}.} pour la détection de mouvements.
\par \texttt{BlasterX Senz3D} a fait son apparation en Septembre 2016. Conçue par Creative,  elle a été présentée comme une webcam intelligente basée sur ce qu'il y a de mieux en matière de savoir-faire et de technologie. Elle possède trois lentilles pour capturer les données visuelles: une caméra RVB, une caméra infra-rouge et un projecteur laser. Ces dernières collaborent avec la technologies Inetl RealSense pour réagir aux expressions faciales et aux gestes corporels des utilisateurs.
\par \texttt{Orbbec Astra Pro} fait partie de la série Astra. Elles offres une vision par ordinateur qui permet des dizaines de fonctions telles que la reconnaissance des visages, la reconnaissance des gestes, le suivi du corps humain, la mesure tridimensionnelle, la perception de l'environnement et la reconstruction de cartes en trois dimensions.  De plus, elles offres une réactivité haut de gamme, une mesure de la profondeur, des dégradés fluides et des contours précis, ainsi que la possibilité de filtrer les pixels de profondeur de faible qualité.
\par \texttt{Intel RealSense D415} a un champ de vision standard bien adapté aux applications de haute précision telles que la numérisation 3D. Elle comprend le processeur Intel RealSense Vision D4 offrant une résolution en profondeur élevée, des capacités de longue portée, une technologie d'obturation globale et un large champ de vision. Grâce à ces deux derniers, la caméra offre une perception précise de la profondeur lorsque l'objet est en mouvement ou que l'appareil est en marche. De plus, elle couvre un champ de vision plus large, minimisant ainsi les angles morts.

\section{Taux d'erreur (RMSE)}

\section{Théoriquement (RMSE)}

<<<<<<< HEAD
\section{Erreur de mesure}
L'erreur de mesure, autrefois appelée l'erreur absolue, est un processus qui permet d'évaluer l'écart entre la valeur mesurée et la valeur de référence qui est soit exacte ou connue. L'objectif de la mesure d'erreurs est de jauger à quel degré les deux valeurs sont proches. 
Dans cette section, nous citons les types ainsi que quelques sources d'erreurs, ensuite,  nous faisons un détour sur les moyens théoriques et physiques employés à cet effet.
\subsection{Types et sources d'erreurs}
Il existe deux types d'erreurs: aléatoires et systématiques. \\ Les erreurs aléatoires sont des erreurs dont les valeurs sont incohérentes et imprévisibles même en répétant les observations et en conservant les mêmes paramètres. \\Les erreurs dites systématiques, à l'inverse des erreurs aléatoires, sont des erreurs reproductibles, elles demeurent constantes tant que les paramètres restent inchangés. \\ \\
Ces erreurs peuvent résulter de différentes sources; elles peuvent être dues à des facteurs environnementaux, à la résolution de l'instrument utilisé, au calibrage de l'appareil employé ou même à des erreurs humaines. \\ \\
\subsection{Mesure d'erreurs à l'aide de matériels}
\subsection{Mesure d'erreurs à l'aide de moyens théoriques}



%\section{Techniques existantes de capture de mouvements} 
\section[Technologies utilisées pour la depth]{Technologies utilisées pour la depth d'une caméra RGB-D} 
Des méthodes employées pour la récupération de la depth, on peut en citer deux principales.
\subsection{Infrarouge}
Le projecteur infrarouge de certaines caméras le possédant qu'on peut voir sur la figure \ref{fig-Asus} projette un spectre infrarouge sur la scène captée. Le motif produit sur cette dernière sera capté par la caméra infrarouge et sera par la suite comparé à la base de données de motifs de référence stockés au préalable dans la caméra. Ces derniers seront indispensables pour la mesure de la profondeur de chaque pixel. 

\begin{figure}[htbp]
  %\centering
  \hspace{-0.75cm}
 \includegraphics[scale=0.50]{images/Asus.png} \hspace{2cm}
  \caption{Composants caméra RGB-D (Ici Asus Xtion Pro Live).\label{fig-Asus}}
\end{figure}

Par la suite, les valeurs obtenues seront corrélées à un capteur RGB qu'on peut apercevoir sur la figure. Ces données pourront être représentées par un nuage de points \footnote{Est une représentation des points de coordonnées tridimensionnelles dont chacun peut avoir des attributs qui lui ont sont propres.}.


	\par Toutefois, ce type de caméras possède certaines restrictions. Parmi ces dernières, on peut citer:
	\begin{itemize}
		\item {Distances de mesure limitées.}	
		\item {Problèmes de calculs des informations de profondeur à l'encontre de surfaces brillantes, très mates, transparentes, réfléchissantes ou encore envers des objets absorbants.} 
		\item {Interférence des motifs (patterns) infrarouges si présence de plusieurs caméras RGB-D de même type. En effet, chaque capteur visualisera ses propres motifs ainsi que ceux des autres caméras présentes et ne saura distinguer les siens des autres qui se chevauchent.  De cela découle une perte considérable d'informations de profondeur comme nous pouvons l'observer sur la figure \ref{fig-interference}. 

\begin{figure}[htbp]
  %\centering
  \hspace{0.75cm}
 \includegraphics[scale=0.5]{images/interference.png}
  \caption{(De gauche à droite) image renvoyée par la caméra RGB, image IR, depth map (une seule caméra), depth map (deux caméras). (figure tirée de l'article de \emph{F.Alhwarin, A.Ferrein} et \emph{I.Scholl} ayant comme titre \emph{IR Stereo Kinect: Improving Depth Images by Combining StructuredLight with IR Stereo}).\label{fig-interference}}
\end{figure}

Cependant, des travaux de recherches ont été conduits afin d'y remédier. Parmi eux, on peut citer le travail réalisé par l'équipe de \emph{Rafibakhsh} ( \cite{RAFIBAKHSH15}) qui recommande de laisser un angle de 35\textdegree{} entre deux caméras suspendues à la même hauteur en considérant les scènes captées dans de bonnes conditions et une interférence très faible. \emph{Maimone} et \emph{Fuchs} \cite{MaimoneFuchs15} proposent un algorithme de remplissage et de lissage en modifiant le filtre médian aux zones trouées à l'exception des bords. Quant à \emph{F. Kenton Musgrave, Craig E.} et \emph{Robert S. Mace} \cite{KentonCraigMace12}, ils appliquent une certaine quantité minime de mouvements (en utilisant des composants matériels supplémentaires) à certains capteurs de sorte que chacun puisse voir son propre motif infrarouge de façon nette et une version floue des motifs de ses voisins. }
	\end{itemize}
\subsection{Stéréo-vision}
Une caméra stéréoscopique est un appareil qui contient deux voire plus de capteurs d'images. Ceci nous rappelle la vision binoculaire humaine. En effet, ce mécanisme permet au système nerveux central de percevoir simultanément les images issus de chaque œil envoyées sous forme de signaux. Ainsi, il sera en mesure de se servir de ces différences (entre les deux images)  pour permettre une vision stéréoscopique pour la perception de relief et une mesure des distances en utilisant la triangulation \footnote{Approche géométrique permettant une mesure des distances. Le lecteur peut consulter cette page web pour de plus amples informations: \url{https://fr.wikipedia.org/wiki/Triangulation} .}. Le concept que nous venons d'expliquer est appelé disparité stéréoscopique \footnote{Différence dans la localisation d'un objet perçu par l'œil gauche et l'œil droit résultant de la séparation horizontale des yeux dont certaines caméras essaient de s'approprier la technique afin de récupérer la profondeur.}. 
\par La stéréo-vision sert principalement à reconstituer la scène observée sous forme de modèle 3D. 
\subsubsection{Carte de disparités}
Le principe de la disparité est une approche du mécanisme humain vu précédemment. Il consiste en la différence entre les coordonnées pixels d'un point bidimensionnel d'une image et celles de son correspondant (présent sur une autre image prise au même moment). Ainsi, en appliquant le même traitement sur tous les pixels correspondants, on obtient la carte des disparités. Une carte est dite éparse lorsqu'une disparité est associée à quelques pixels et lorsque cette dernière est associée à chaque pixel, on dit d'elle qu'elle est dense.  \\ Une formule pour calculer la profondeur en fonction de la disparité s'obtient comme suit: \\ \\
\begin{equation}
 z = \dfrac{B.f}{d}  
\end{equation} 
\\ Avec: \\ \\
$z$ représentant la profondeur. \\
$B$ \emph{Baseline} représente la distance séparant les deux capteurs. \\
$f$ La distance focale en pixels calculée comme montré ci-après en nous servant de la figure \ref{fig-focal} . \\
\begin{figure}[htbp]
  %\centering
  \hspace{-3cm}
 \includegraphics[scale=0.5]{images/focalLength.png} \hspace{2cm}
  \caption{Schéma illustrant la relation entre la distance focale et le champ de vision (fov).\label{fig-focal}}
\end{figure}


\begin{equation}
 \tan \left( {\dfrac{fov}{2}}\right) = \dfrac{D}{2.f}    \Leftrightarrow   f = \dfrac{D}{2. \tan \left( {\dfrac{fov}{2}}\right)}
\end{equation}
\\ Avec:\\ \\
$D$  dimension qui peut représenter soit la largeur soit la hauteur en fonction du fov de la caméra employé. \\
$fov$ (Field Of View) représente l'angle du champ de vision (soit horizontal ou vertical). \\
$f$ représente la distance focale.


\section{Kinect V2 et Kinect V3}
\subsection{Kinect V2}
\texttt{Kinect V2} se sert de la méthode TOF (ie: Time Of Flight ou autrement dit Temps De Vol) pour générer la carte de profondeur. Cette technique se base sur la différence de temps entre l'émission d'un faisceau lumineux et son retour après réflexion sur un objet. \\ La distance est calculée comme suit:\\
$ d = c.\dfrac{\Delta t}{2} $, avec $c$ la vitesse de la lumière dans l'air.\\ \par Elle permet une bien meilleure précision même dans le noir que sa version précédente (\texttt{Kinect V1)}. 

\subsection{Kinect V3}
\texttt{Kinect V3}, baptisée \texttt{Microsoft Azure Kinect V3}, tout comme la précédente version, se base aussi sur la technologie TOF. Elle comprend un capteur RGB de 12 Mp, un capteur de depth de 1Mp avec un fov réglable en large ou réduit ainsi que 7 microphones intégrés.  
 
\subsubsection{Kinect V2 Vs Kinect V3}
\texttt{Kinect V3} est beaucoup plus légère et plus petite que la \texttt{V2}. Elle possède entre autre plus de microphones que la précédente. Elle a été conçue afin d'être principalement utilisée avec Azure , le service cloud de \texttt{Microsoft}. C'est sans doute ce qui la démarque le plus de sa prédécesseure. En effet, ce service lui permet d'effectuer une partie des calculs. En outre, elle bénéficie des Cognitive Services, autrement dit de l'intelligence artificielle pourra être incluse dans les applications créées.

\chapter[Notre outil pour la qualification]{Notre outil pour la qualification de caméras RGB-D}
Dans ce passage, nous allons décrire notre outil conçu pour qualifier les caméras par rapport à notre modèle-étalon montré dans l'image ci-dessous.

\begin{figure}[htbp]
  %\centering
  \hspace{0.75cm}
 \includegraphics[scale=0.5]{images/realModel.png} \hspace{2cm}
  \caption{Modèle-étalon.\label{fig-model}}
\end{figure}
=======
\chapter[Modèle logiciel pour la qualification]{Modèle logiciel pour la qualification de caméra RGB-D}
Dans ce passage, nous allons décrire notre outil conçu pour qualifier les caméras.
>>>>>>> e27057fb0e07f9d83f35728f1f643f94249c480e

\section{Description de l'application}

\subsection{Première partie: Affichage de la scène captée }

\subsection{Seconde partie: Interface graphique}


\section{Conception du modèle réel et du modèle virtuel}

\section{Comparaison de la depth OpenGL avec la depth de la caméra RGB-D}
Dans ce passage, nous exposons certains des résultats obtenus ainsi que les résultats attendus avec la caméra Asus.

\subsection{Résultats obtenus}

\subsection{Résultats attendus}


\chapter{Cas pratiques de qualification}

\section{Modèle 3D}
\section{La POC}
\section{Résultats et critique}


\chapter{Conclusion et Perspectives\label{chap-conclusion}}


\nocite{*}
%	\bibliographystyle{alpha}
%\bibliography{memoire}
\printbibliography
\end{document}

